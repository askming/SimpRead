---
title: Google Research 2025: Bolder breakthroughs, bigger impact
url: https://research.google/blog/google-research-2025-bolder-breakthroughs-bigger-impact/
date: 2025-12-31
status: read
---

# Google Research 2025: Bolder breakthroughs, bigger impact

Google Research teams have invested over the years in advancing research and technology in a diverse range of strategic areas. We are working across time horizons, from bold moonshots and curiosity-driven transformative research where we explore the art of the possible, to innovation and applied research with accelerated impact. The [Magic Cycle of research](https://research.google/blog/accelerating-the-magic-cycle-of-research-breakthroughs-and-real-world-applications/) is accelerating — we’re driving research breakthroughs and translating them into real-world solutions, with impact on products, science and society, in close collaboration with many teams across Google and global partners.

This was quite a year! Our foundational AI breakthroughs helped make generative models more efficient, factual, multilingual, and multi-cultural, and we introduced generative UI. We advanced new architectures and algorithmic research and pioneered AI tools and agentic models that help accelerate scientific discovery. We achieved quantum breakthroughs that bring us closer to real-world applications of quantum computing; advanced research on Earth sciences to enable a level of planetary understanding never before possible; drove forward scientific domains including genomics, biology and neuroscience; and made headway on societal priorities like climate resilience, health and education.

![A timeline of Google Research’s 2025 moments](https://storage.googleapis.com/gweb-research2023-media/images/EOY-0d-Hero.width-1250.png)

_A look back at some of Google Research's 2025 moments realized in collaboration with many teams across Google. This image was created with Nano Banana._

## Advancing generative models to be more efficient, factual, multilingual and multi-cultural

To help fuel this era of rapid innovation, we’re investing in efficiency, making Google products more cost and energy efficient, and setting the bar for the industry. We continue to develop new approaches based on [speculative decoding](https://research.google/blog/looking-back-at-speculative-decoding/), such as [block verification](https://arxiv.org/abs/2403.10444), to further accelerate efficiency gains. At the other end of the infrastructure stack, [LAVA](https://research.google/blog/solving-virtual-machine-puzzles-how-ai-is-optimizing-cloud-computing/) is a new scheduling algorithm that continuously re-predicts the lifespans of tasks on virtual machines. It is designed to optimize resource efficiency in large cloud data centers, without sacrificing reliability.

Equally critical, our pioneering research on LLM factuality, dating back to 2021, helps make [Gemini 3](https://blog.google/products/gemini/gemini-3/#gemini-3) our most capable and factual LLM yet. It achieves state-of-the-art performance on public factuality benchmarks like [SimpleQA Verified](https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified) and the new [FACTS benchmark suite](https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/) that we released with Google DeepMind and Kaggle. Users can be confident that products such as the Gemini app, [AI Overviews](https://blog.google/products/search/generative-ai-google-search-may-2024/) and [AI Mode](https://blog.google/products/search/google-search-ai-mode-update/#ai-mode-search) in Search, and Vertex AI all provide outputs grounded in world knowledge. This year we studied how LLMs [convey uncertainty](https://arxiv.org/abs/2505.24858); presented a framework for assessing whether LLMs [encode more factual knowledge](https://arxiv.org/abs/2503.15299) in their parameters than they express in their outputs; presented a multilingual dataset that evaluates cross-lingual knowledge, called [ECLeKTic](https://research.google/blog/eclektic-a-novel-benchmark-for-evaluating-cross-lingual-knowledge-transfer-in-llms/); and more.

We also explored the role of [sufficient context](https://research.google/blog/deeper-insights-into-retrieval-augmented-generation-the-role-of-sufficient-context/) in [retrieval augmented generation](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) systems, which enhance LLMs by providing them with relevant external context. We [demonstrated](https://arxiv.org/abs/2411.06037) that it is possible to know when an LLM has enough information to provide a correct answer to a question. This work supported the launch of the [LLM Re-Ranker](https://cloud.google.com/vertex-ai/generative-ai/docs/retrieval-and-ranking#llm_reranker) in the [Vertex AI RAG Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/rag-overview), leading to better retrieval metrics and system accuracy.

_We evaluated leading LLMs on the_ _[FACTS Benchmark Suite_](https://deepmind.google/blog/facts-benchmark-suite-systematically-evaluating-the-factuality-of-large-language-models/)_, which includes four different factuality benchmarks. The table above lists 15 leading models and their overall FACTS score. Gemini 3 Pro leads in overall performance._

With the rise of multimodal content, we’ve expanded our work on factuality to images, audio, video, 3D environments and LLM-generated applications. This work helps to improve the quality of Google’s video and image model families, including [Veo](https://deepmind.google/models/veo/), [Imagen](https://blog.google/products/gemini/gemini-app-updates-io-2025/#imagen-4-veo-3) and [Nano Banana](https://blog.google/products/gemini/updated-image-editing-model/). It is a great example of the cycle of research and how we’re continuously adapting to real user needs. Our latest research includes making [text-to-image generation](https://arxiv.org/abs/2504.17502) and [image captions](https://arxiv.org/pdf/2506.07631) more accurate, and creating [3DMem-Bench](https://arxiv.org/abs/2505.22657) for evaluating an agent’s ability to reason over long-term memory in 3D.

Our long-running multilinguality research helped [Gemma](https://deepmind.google/models/gemma/) expand to over 140 languages, making it today’s best multilingual open model. We’re also augmenting our models with socio-cultural intelligence, attuning them to diverse user needs and global contexts. We introduced [TUNA](https://arxiv.org/abs/2510.06124), a comprehensive taxonomy of user needs and actions, launched a community-based data [collection platform](https://research.google/blog/amplify-initiative-localized-data-for-globalized-ai/) to target under-represented languages and geographies, and developed new methods to ground models in [diverse cultural knowledge](https://arxiv.org/pdf/2502.13497) and datasets. This research helps to ensure that Google models can connect with users globally in responsible and culturally-aware ways.

## Introducing interactive interfaces with generative UI

In a world where users expect more engaging and visual experiences, we introduced a novel implementation of [generative UI](https://research.google/blog/generative-ui-a-rich-custom-visual-interactive-user-experience-for-any-prompt/) in Gemini 3. This powerful capability enables AI models to dynamically create immersive visual experiences and interactive interfaces, such as web pages, games, tools and apps, in response to a prompt. Our research comes to life in AI Mode on [Google Search](http://blog.google/products/search/gemini-3-search-ai-mode), and in experiments such as dynamic view, in the [Gemini app](https://blog.google/products/gemini/gemini-3-gemini-app).

![Video preview image](https://i.ytimg.com/vi_webp/JALZmOVlR7s/maxresdefault.webp)

## Watch the film

_Example of generative UI capabilities in dynamic view in the Gemini app. This is based on the prompt, “_Create a Van Gogh gallery with life context for each piece._” More examples can be found_ _[here_](https://generativeui.github.io/)_._

_Example of generative UI capabilities in AI Mode in Google Search. This is based on the prompt, “_Show me how rna polymerase works. what are the stages of transcription and how is it different in prokaryotic and eukaryotic cells._”_

## Quantum computing: The next frontier

Our strategic investment in quantum computing is poised to accelerate the next frontier of computing and scientific discovery. In the 1980s, Clarke, Devoret, and Martinis laid the foundations for superconducting qubits, which led to their recognition as [2025 Physics Nobel Laureates](https://blog.google/inside-google/company-announcements/googler-michel-devoret-awarded-the-nobel-prize-in-physics/). The 40-year [journey](https://quantumai.google/roadmap) since has yielded the nascent quantum computing industry and led to breakthroughs like our recently announced [verifiable quantum advantage](https://research.google/blog/a-verifiable-quantum-advantage/), published on the cover of _[Nature_](https://www.nature.com/articles/s41586-025-09526-6). This work describes our “[Quantum Echoes](https://blog.google/technology/research/quantum-echoes-willow-verifiable-quantum-advantage/)” algorithm, which runs on our [Willow chip](https://research.google/blog/making-quantum-error-correction-work/) 13,000 times faster than the best classical algorithm on one of the world’s fastest supercomputers. It offers a new way to explain interactions between atoms in a molecule observed [using nuclear magnetic resonance spectroscopy](https://arxiv.org/abs/2510.19550). It brings us closer to [real-world applications](https://blog.google/technology/research/useful-quantum-computing-applications/) of quantum computing, such as advancing drug design and helping to make fusion energy a reality.

![Video preview image](https://i.ytimg.com/vi_webp/lpzR_rPbrac/maxresdefault.webp)

## Watch the film

## Accelerating scientific discovery

AI-powered models and platforms are fundamentally changing _how_ science is conducted. We released [AI co-scientist](https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/), a collaboration across Google Research, Cloud AI and Google DeepMind. This multi-agent [AI system](https://arxiv.org/abs/2502.18864) helps scientists generate novel hypotheses. We also shared our [AI-powered empirical software](https://research.google/blog/accelerating-scientific-discovery-with-ai-powered-empirical-software/) system, a Gemini-backed coding agent to help scientists write expert-level empirical software to evaluate and iterate on hypotheses. These tools accelerate the very process of making scientific discoveries. They open the door to a future where every scientist in a lab has a team of AI assistants simultaneously investigating thousands of potential solutions to the scientific challenges that motivate their research. Already at Stanford, our AI co-scientist has helped [identify drugs](https://advanced.onlinelibrary.wiley.com/doi/pdf/10.1002/advs.202508751) that could be repurposed to treat liver fibrosis. At Imperial College London, researchers working on antimicrobial resistance [found](https://www.imperial.ac.uk/news/261293/googles-ai-co-scientist-could-enhance-research/) that it produced the same hypothesis in days that their team took years to develop.

_An overview of the AI co-scientist. It uses a coalition of specialized agents who iteratively generate, evaluate, and refine hypotheses._

## Advancing science — from biology to genomics to neuroscience

We continue to advance core scientific research. [DeepSomatic](https://research.google/blog/using-ai-to-identify-genetic-variants-in-tumors-with-deepsomatic/) and [C2S-Scale](https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/) join the AI-powered fight against cancer and are paving the way for brand-new therapies. Published in _[Nature Biotechnology_](https://www.nature.com/articles/s41587-025-02839-x)_,_ DeepSomatic is an [open-source](https://github.com/google/deepsomatic) tool that builds on [10 years of genomics](https://blog.google/technology/research/ten-years-google-genomics/) research at Google and helps scientists and doctors identify genetic variants in cancer cells. Our partners at [Children’s Mercy](https://www.childrensmercy.org/childrens-mercy-research-institute/about/) are using it to understand [how and why a particular form of cancer](https://www.medrxiv.org/content/10.1101/2024.11.05.24316078v1) affects a patient in order to develop personalized cures. C2S-Scale, which we released in collaboration with Google DeepMind and Yale, is a 27 billion parameter foundation model for single-cell analysis that made headlines for generating a [novel hypothesis](https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/) about cancer cellular behavior.

Turning to neuroscience, we [published in _Nature_](https://www.nature.com/articles/s41586-025-08985-1) the first-ever method for using commonly available light microscopes to comprehensively map all the neurons and their connections in a block of brain tissue. Working with the [Institute of Science and Technology Austria](https://ista.ac.at/en/home/), we applied our suite of image analysis and ML tools for [connectomics](https://sites.research.google/neural-mapping/), leveraging over a decade of contributions we’ve made to this scientific field to understand the workings of the brain. We hope the method, called [LICONN](https://research.google/blog/a-new-light-on-neural-connections/), will enable more labs around the world to pursue connectomics studies.

We also [open-sourced](https://zapbench-release.storage.googleapis.com/landing.html) the [Zebrafish Activity Prediction Benchmark](https://research.google/blog/improving-brain-models-with-zapbench/) (ZAPBench) in collaboration with [HHMI Janelia](https://www.janelia.org/) and [Harvard](https://lichtmanlab.fas.harvard.edu/). With recordings of more than 70,000 neurons from the larval zebrafish brain, it will enable scientists to [investigate the relationship](https://arxiv.org/abs/2503.02618) between the structural wiring and dynamic neural activity across an entire vertebrate brain for the first time.

Plus, we demonstrated how LLMs can help us understand the human brain. In a [series](https://www.nature.com/articles/s41593-022-01026-4) of [studies](https://www.nature.com/articles/s41467-024-46631-y) conducted over five years with [Princeton University](https://hassonlab.princeton.edu/), [NYU](https://nyulangone.org/locations/comprehensive-epilepsy-center), and [HUJI](https://www.deepcognitionlab.com/), we explored connections in the ways the human brain and deep language models [process natural language](https://research.google/blog/deciphering-language-processing-in-the-human-brain-through-llm-representations/). We discovered [remarkable alignment](https://www.nature.com/articles/s41562-025-02105-9) between the neural activity in the speech and language areas of the human brain and the speech and language embeddings of a Transformer-based speech-to-text model, and showed how the [temporal structure](https://www.nature.com/articles/s41467-025-65518-0.epdf?sharing_token=XqLw7uzRcb-8uuak6fH8BdRgN0jAjWel9jnR3ZoTv0PSBTWDWtB7_YH7achJWXIc1XVbsxtqM_vvMoZPLZuQIoT45xjjIkxPMOnKorePnnoS9QL1SRZ58ZI65TPosI2w5PF8h5ZazOVlXhxgrnGSxw1GguylV5BBtKxLUyCAfLM%3D) of language processing in the brain corresponds to the layered hierarchy of deep language models. Our research indicates that language representation in deep learning models could offer a novel framework for understanding the brain’s neural code; it also paves the way for innovative approaches to creating artificial neural networks with better information processing capabilities.

## Enabling planetary intelligence and crisis resilience

[Earth AI](https://research.google/blog/google-earth-ai-unlocking-geospatial-insights-with-foundation-models-and-cross-modal-reasoning/) is Google’s family of geospatial AI models and reasoning agents that provides users with actionable insights, grounded in real-world understanding. Developed in collaboration with teams across Google, it builds on our years of modeling the world, paired with Gemini’s advanced reasoning, to offer an unprecedented level of understanding about our planet. It [brings together](https://research.google/blog/google-earth-ai-unlocking-geospatial-insights-with-foundation-models-and-cross-modal-reasoning/) many of Google’s geospatial models and technologies such as [remote sensing imagery](https://research.google/blog/google-earth-ai-unlocking-geospatial-insights-with-foundation-models-and-cross-modal-reasoning/), [weather](https://research.google/blog/fast-accurate-climate-modeling-with-neuralgcm/), [air quality](https://blog.google/products/maps/google-maps-apis-environment-sustainability/), [floods](https://sites.research.google/gr/floodforecasting/), [population dynamics](https://research.google/blog/insights-into-population-dynamics-a-foundation-model-for-geospatial-inference/), [AlphaEarth Foundations](https://deepmind.google/discover/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/), [mobility](https://research.google/blog/introducing-mobility-ai-advancing-urban-transportation/), [maps](https://blog.google/technology/research/open-buildings-ai-powered-maps-for-a-changing-world/) and more. Thanks to Gemini’s reasoning power, Earth AI can synthesize vast datasets about the planet to generate insights in minutes that would previously take years of research. Earth AI offerings are available in [Google Maps Platform](https://mapsplatform.google.com/), [Google Earth and to Trusted Testers via Google Cloud](https://blog.google/technology/research/new-updates-and-more-access-to-google-earth-ai/), and are already being used by partners, helping cities, enterprises and nonprofits with critical tasks from [urban planning](https://www.youtube.com/watch?v=ZxmB8Z5i1Ls&list=PL95lT3XlM14ROFtYnlBDYZbKipH3JLaC7&index=3) to [disaster response](https://www.youtube.com/watch?v=8-macH8ozr4&list=PL95lT3XlM14ROFtYnlBDYZbKipH3JLaC7&index=5).

![Video preview image](https://i.ytimg.com/vi_webp/UZ4RaLGDXI4/maxresdefault.webp)

## Watch the film

We’ve also made significant strides with the climate models that feed our AI capabilities for understanding the Earth, helping communities to prepare for and respond to severe weather and natural disasters. This year, in collaboration with the [Earth Fire Alliance](https://www.earthfirealliance.org/), the [Moore Foundation](https://www.moore.org/) and [Muon Space](https://www.muonspace.com/), we launched the first satellite in the [FireSat](https://sites.research.google/gr/wildfires/firesat/) constellation. Named as one of [TIME magazine’s best inventions](https://time.com/collections/best-inventions-2025/7318230/firesat/) of 2025, FireSat uses AI to provide critical near–real-time insights for first responders. It has already [detected](https://blog.google/technology/research/first-firesat-images/) small wildfires not caught by other space-based systems, and when fully operational with over 50 satellites, it will be able to detect a classroom-sized wildfire anywhere on Earth.

_FireSat is equipped with a custom mid-wave infrared (MWIR) sensor that detected a small, relatively cool roadside fire near Medford, Oregon that was not detected by other space-based systems. It is seen here overlaid on a Google Earth basemap. Credit: Muon Space and Earth Fire Alliance._

We also [expanded](https://blog.google/technology/research/new-updates-and-more-access-to-google-earth-ai/) our flood forecasting models to cover over 2 billion people in 150 countries for the most significant riverine flood events, helping communities stay safe and informed. We partnered with our colleagues at Google DeepMind to debut an experimental model for [cyclone predictions](https://deepmind.google/blog/how-were-supporting-better-tropical-cyclone-prediction-with-ai/) using stochastic neural networks that's helping weather agencies predict a cyclone’s path up to 15 days in advance. Moreover, we collaborated with Google DeepMind to launch [WeatherNext 2](https://deepmind.google/science/weathernext/), which delivers our most accurate, mid-range AI weather forecasts to date. It’s now available to users of Search, Gemini and Pixel Weather as well as to developers on Google Maps and Google Cloud.

At the start of the year, we expanded [Nowcasting on Search to Africa](https://blog.google/intl/en-africa/products/explore-get-answers/nowcasting-on-search-is-bringing-ai-powered-weather-forecasts-to-users-across-africa/), bringing highly precise, short-term precipitation forecasts to users across the continent for the first time. We have since made this available for users worldwide. Powered by our [MetNet](https://arxiv.org/abs/2510.13050) model, it represents the first AI weather model on Search to operate at a global scale. In India, the University of Chicago and the Indian Ministry of Agriculture and Farmers’ Welfare used Google’s [NeuralGCM model](https://blog.google/technology/research/indian-farmers-monsoon-prediction/) to send longer-range monsoon forecasts to 38 million farmers, helping them make critical decisions about what to plant and when.

## Advancing Health AI

As we make scientific breakthroughs with the potential to significantly reform healthcare, we’re working with partners and healthcare professionals to bring new capabilities responsibly to people around the world. [AMIE](https://research.google/blog/amie-a-research-ai-system-for-diagnostic-medical-reasoning-and-conversations/) is our conversational medical agent developed together with Google DeepMind and published in _[Nature_](https://www.nature.com/articles/s41586-025-08866-7). It can now reason through [multimodal](https://research.google/blog/amie-gains-vision-a-research-ai-agent-for-multi-modal-diagnostic-dialogue/) evidence and support longitudinal [disease management](https://research.google/blog/from-diagnosis-to-treatment-advancing-amie-for-longitudinal-disease-management/) as well as or better than primary care physicians under simulated settings with professional patient actors. We’re exploring how this research could enable a physician-centered model with [asynchronous oversight](https://research.google/blog/enabling-physician-centered-oversight-for-amie/) of AMIE. We also launched [Plan for Care Lab](https://community.fitbit.com/t5/The-Pulse-Fitbit-Community-Blog/Introducing-the-new-Plan-for-Care-Fitbit-Lab/ba-p/5796289), Fitbit’s latest experimental capability, to a select number of opt-in users. It’s designed to help users access personalized support when assessing symptoms at home and preparing for an upcoming doctor’s visit. In addition, [MedGemma](https://research.google/blog/medgemma-our-most-capable-open-models-for-health-ai-development/), Google's most capable open model for multimodal medical comprehension, is available as part of our [Health AI Developer Foundations](https://developers.google.com/health-ai-developer-foundations) (HAI-DEF). It can support tasks such as classification, report generation, or interpreting complex electronic health records, making it useful for medical research and product development. Since launch, MedGemma and HAI-DEF have >2M downloads. Plus, our [Open Health Stack](https://developers.google.com/open-health-stack) was recognized at the [World Economic Forum](https://www.weforum.org/stories/2025/01/social-innovation-has-moved-from-the-margins-to-the-mainstream/) for helping to address inequities in health access. It provides the building blocks for developers to create next-gen, data-driven healthcare apps for use in low-resource settings.

## Advancing learning and education

Gemini is now infused with [LearnLM](https://blog.google/outreach-initiatives/education/google-learnlm-gemini-generative-ai/), Google’s family of models fine-tuned for learning, [announced](https://storage.googleapis.com/deepmind-media/LearnLM/LearnLM_paper.pdf) last year. We launched [Learn Your Way](https://research.google/blog/learn-your-way-reimagining-textbooks-with-generative-ai/) on [Google Labs](https://learnyourway.withgoogle.com/), powered by LearnLM’s foundational capabilities. It explores the future of textbooks by generating multiple engaging representations of the source material. It transforms static textbooks into active learning experiences that are tailored for every student, with interactive quizzes that enable real-time assessment, feedback, and content personalization. In our[ efficacy study](https://arxiv.org/abs/2509.13348), students using it scored 11 percentage points higher on retention tests. We also piloted our LearnLM model for [answer assessment](https://blog.google/intl/en-africa/company-news/outreach-and-initiatives/5-ways-were-bringing-ai-innovations-to-people-across-africa/) with thousands of high school students in Ghana. Plus, we explored the intersection of education and health through a learner-centric approach quantifying the benefits of LearnLM in [medical education](https://research.google/blog/how-googles-ai-can-help-transform-health-professions-education/) settings.

This research brings us closer to realizing a future where AI makes learning more effective for everyone. In collaboration with teams across Google, we published “[AI and the Future of Learning](https://services.google.com/fh/files/misc/future_of_learning.pdf)”, sharing our approach, grounded in learning science, to responsibly enable AI for learning. We’re creating personalized teaching experiences, empowering educators, and working to address challenges such as critical thinking and equal access.

In parallel, our AI Literacy efforts aim to inspire the next generation of innovators. [AI Quests](https://research.google/ai-quests/intl/en_gb), [launched](https://blog.google/outreach-initiatives/education/ai-quests/) with the [Stanford Accelerator for Learning](https://acceleratelearning.stanford.edu/), allows students to step into the shoes of Google researchers and use AI to solve challenges like flood forecasting and detecting eye disease. During [Computer Science Education Week](https://blog.google/outreach-initiatives/education/next-gen-computer-science-innovators/), hundreds of Googler volunteers brought these quests to classrooms around the world.

_Learn Your Way explores how GenAI can transform educational materials into more effective, engaging, learner-driven experiences. It generates multiple representations of the source material, tailored for each student._

## Advancing ML foundations and algorithmic research

Our broad foundational ML and algorithmic research is the bedrock for groundbreaking advances across domains. This work provides the essential frameworks that power products and services, and underpins the development of next-generation models and intelligent systems. We improved voice search, for example, with our new [Speech-to-Retrieval](https://research.google/blog/speech-to-retrieval-s2r-a-new-approach-to-voice-search/) engine, which directly interprets and retrieves information from a spoken query without having to convert it first to text. And our state-of-the-art [predictive modeling](https://research.google/blog/rich-human-feedback-for-text-to-image-generation/) of rich human feedback improved text-to-image generation quality in products, including [Imagen3](https://developers.googleblog.com/en/imagen-3-arrives-in-the-gemini-api/), [creative generation](https://blog.google/products/ads-commerce/new-creative-updates-advertisers-generate-lifestyle/) and [editing](https://blog.google/products/ads-commerce/google-ai-ads-creative/) in Google Ads, and [virtual try on](https://blog.google/products/shopping/google-shopping-ai-mode-virtual-try-on-update/) for shopping. We also extended this research to improve video generation quality in the [Wizard of Oz film launch](https://blog.google/products/google-cloud/sphere-wizard-of-oz/) at [Sphere](https://www.thesphere.com/shows/wizard-of-oz-experience) in Las Vegas.

The impact of our algorithmic research extends well beyond Google products. Our [TimesFM](https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/) model, which [helps businesses](https://cloud.google.com/blog/products/data-analytics/timesfm-models-in-bigquery-and-alloydb) with time-series forecasting, now has hundreds of millions of queries per month in [BigQuery](https://cloud.google.com/bigquery) and [AlloyDB](https://cloud.google.com/products/alloydb). We introduced a novel approach using [in-context fine-tuning](https://research.google/blog/time-series-foundation-models-can-be-few-shot-learners/), which teaches the model how to learn from multiple examples at inference time to further enhance its performance. Our [Mobility AI](https://research.google/blog/introducing-mobility-ai-advancing-urban-transportation/) model leverages our two decades of innovation in maps and transportation to provide transportation agencies with powerful tools for data-driven policymaking and traffic management. It can understand traffic and parking patterns, simulate systems to allow engineers to test different scenarios, and identify effective solutions for transportation networks. This complements our consumer-facing breakthroughs in Google Maps and Search, such as specialized models for [calculating ETAs](https://research.google/blog/how-we-created-hov-specific-etas-in-google-maps/) and [optimizing trip planning](https://research.google/blog/optimizing-llm-based-trip-planning/).

_Our Mobility AI Traffic Simulation API is built for modeling complex, city-scale traffic scenarios. The tool provides high-fidelity simulations of road closures, helping to de-risk major infrastructure investments and validate emergency response plans. It was launched in Seattle, Denver, Boston, Philadelphia and Orlando. The above image shows an example simulation in Seattle. Watch the full demo_ _[here_](https://www.youtube.com/watch?v=Pv91I43VpOQ)_._

Additionally, we’ve explored a range of topics in economics and computation from pricing dynamics in [modular marketplaces](https://arxiv.org/abs/2502.20346) and in [procurement auctions](https://arxiv.org/abs/2503.10910), to [data-driven mechanism design](https://dl.acm.org/doi/10.1145/3736252.3742578) and [various](https://dl.acm.org/doi/10.1145/3696410.3714881) [approaches](https://dl.acm.org/doi/10.1145/3736252.3742545) to optimize ad auctions. We also studied swap regret and [correlated equilibria in games](https://arxiv.org/abs/2502.20229).

As AI becomes increasingly integrated into our daily lives, building it with privacy at its core is critical for users and industries. To this end, we’ve developed and published [novel](https://research.google/blog/fine-tuning-llms-with-user-level-differential-privacy/) [algorithms](https://research.google/blog/beyond-billion-parameter-burdens-unlocking-data-synthesis-with-a-conditional-generator/) for [private](https://research.google/blog/synthetic-and-federated-privacy-preserving-domain-adaptation-with-llms-for-mobile-applications/) learning and [private](https://research.google/blog/securing-private-data-at-scale-with-differentially-private-partition-selection/) [analytics](https://research.google/blog/toward-provably-private-insights-into-ai-use/), and open sourced robust software tools to enable [external verifiability](https://research.google/blog/discovering-new-words-with-confidential-federated-analytics/). For example, we introduced [Parfait](https://research.google/blog/parfait-enabling-private-ai-with-research-tools/), a new GitHub organization for businesses and open-source projects. It has supported Google deployments of federated learning and analytics from [Gboard](https://support.google.com/gboard/answer/12373137?hl=en#zippy=%2Cfederated-learning) to [Google Maps](https://arxiv.org/abs/2412.07962). We also announced [Jax Privacy 1.0](https://research.google/blog/differentially-private-machine-learning-at-scale-with-jax-privacy/), a library for ML with differential privacy, which we used to train [VaultGemma](https://research.google/blog/vaultgemma-the-worlds-most-capable-differentially-private-llm/), the largest and most capable open model trained from scratch with differential privacy, with weights available on [Hugging Face](https://huggingface.co/google/vaultgemma-1b) and [Kaggle](https://www.kaggle.com/models/google/vaultgemma). By leveling up our privacy capabilities, we offer much stronger protections to businesses and users

## Introducing novel architectures

Our foundational ML research introduces advanced approaches to enable new opportunities. [Nested Learning](https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/) is a new ML paradigm that represents a leap forward in our understanding of deep learning. It treats model architecture and optimization as a single system that contains several, smaller, nested optimization problems. By unifying these elements, it solves the problem of catastrophic forgetting, when LLMs become forgetful and less capable at old tasks after learning new tasks. This research could help us build the next generation of more capable, self-improving AI. Meanwhile, our [Titans architecture and the MIRAS framework](https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/) mark a significant advancement in sequence modelling. They allow AI models to work much faster and handle massive contexts by employing deep neural networks that learn to memorize as data comes in, improving AI’s long-term memory.

_In the MIRAS framework, we aim to learn an associative memory, mapping between keys and values. For each token, the memory module internally optimizes its inner attentional bias while using its retention gate to make sure that it does not deviate from its past state._

We also introduced [MUVERA](https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/), a novel retrieval algorithm that reduces complex multi-vector retrieval back to single-vector maximum inner product search, achieving state-of-the-art performance with significantly improved efficiency. It creates new possibilities for information retrieval for use in applications such as recommendation systems and natural language processing. And our progress on [graph foundational models](https://research.google/blog/graph-foundation-models-for-relational-data/) pushes the frontiers of graph learning. While most graph neural networks are fixed to a specific graph on which the model has been trained, we developed graph foundational models capable of generalizing to arbitrary tables, features and tasks. This opens up new avenues for model reuse.

## Collaborating with the research ecosystem

We partner with the academic community, industry leaders, governments and scientific institutes around the world. We also continue to engage the ecosystem through our Research@ events from [Mountain View](https://research.google/blog/accelerating-the-magic-cycle-of-research-breakthroughs-and-real-world-applications/) to [Tokyo](https://www.youtube.com/watch?v=sikTOH-0J_c), [Sydney](https://blog.google/intl/en-au/company-news/technology/research-sydney-charting-new-ai-frontiers-alongside-the-research-ecosystem-in-australia/) and [Poland](https://blog.google/technology/research/ai-collaboration-poland-2025/), and we support hundreds of PhD students in Google’s [Fellowship Program](https://blog.google/outreach-initiatives/google-org/phd-fellowship-program-2025/).

As a global team, we continue to expand our footprint beyond our major hubs. Having solidified our research [investment](https://blog.google/intl/en-africa/company-news/outreach-and-initiatives/supporting-the-future-of-ai-research-in-africa-and-globally/) and [innovation](https://blog.google/intl/en-africa/company-news/outreach-and-initiatives/5-ways-were-bringing-ai-innovations-to-people-across-africa/#:~:text=Jul%2024%2C%202025,Mail) in Africa (Accra and Nairobi) and our presence in Australia, we are now preparing to inaugurate a new Google Research hub in Singapore in 2026.

We share our work through publications, conferences, academic talks, benchmarks, datasets and open-source releases. We’ve [sponsored and hosted workshops at conferences](https://research.google/conferences-and-events/?&year=2025), most recently at [NeurIPS](https://research.google/conferences-and-events/google-at-neurips-2025/). We recently introduced an [experimental program](https://research.google/blog/gemini-provides-automated-feedback-for-theoretical-computer-scientists-at-stoc-2026/) that provided automated feedback to scientists before they submit their conference papers for peer review, helping them to rigorously verify their work and accelerate research workflows. Plus, we launched [Google Research Featured Notebooks](https://notebooklm.google.com/notebook/24d50377-8c14-4851-bcc2-b2d67b039041) in collaboration with NotebookLM, to make research more accessible to a broader community.

## AI as an amplifier of human ingenuity

This is a golden age for research. Never before have technical breakthroughs and scientific progress so quickly materialized into impactful, real-world solutions, which, in turn, bring to the fore new data and questions that inspire new avenues of foundational research. This [magic cycle](https://research.google/blog/accelerating-the-magic-cycle-of-research-breakthroughs-and-real-world-applications/) is accelerating significantly, propelled by more powerful models, new agentic tools that support scientific discovery, and open platforms and tools.

Together with our Google colleagues and partners, we’re advancing research and technologies that aim to be helpful in diverse areas. Our research, grounded in a rigorous dedication to safety and trust, serves to unlock human potential — whether that’s to help a scientist accelerate their research, or a student learn more effectively and master new concepts, or to empower a doctor, developer or teacher.

It is truly an exciting time to be in research. We’re able to leverage the full stack of Google AI infrastructure, models, platforms, and world-class talent, and contribute to products used by billions. We will keep building on our legacy, asking the biggest questions of today, and aiming to enable the solutions of tomorrow. We’ll keep advancing AI in a bold and responsible way, for the benefit of society, to help enhance human capacity and make AI an amplifier of human ingenuity.

![Video preview image](https://i.ytimg.com/vi_webp/VqSrYdDM0Zw/maxresdefault.webp)

## Watch the film

## Acknowledgements

_With thanks to everyone in Google Research, and many collaborators, who have contributed to this blog and the work represented here._
